import streamlit as st
import os
import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
import langchain
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# è¨­å®šé é¢è³‡è¨Š
st.set_page_config(page_title="RAG èª²ç¨‹åŠ©ç† (Geminiç‰ˆ)", page_icon="ğŸ“š")
st.title("ğŸ“š RAG å­¸è¡“èª²ç¨‹åŠ©ç†")
st.caption("åŸºæ–¼ Google Gemini èˆ‡ LangChain çš„æª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±")

# Sidebar: API Key è¨­å®š
with st.sidebar:
    st.header("è¨­å®š")
    google_api_key = st.text_input("è¼¸å…¥ Google Gemini API Key", type="password")
    st.markdown("[å–å¾— Google API Key](https://aistudio.google.com/app/apikey)")
    st.markdown("---")
    st.write("æœ¬ç³»çµ±ç”±ç”Ÿæˆå¼ AI èª²ç¨‹å°ˆé¡Œå¯¦ä½œå»¶ä¼¸ã€‚")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼è«‹ä¸Šå‚³ä¸€ä»½ PDF è¬›ç¾©æˆ–è«–æ–‡ï¼Œæˆ‘å¯ä»¥å›ç­”ç›¸é—œå•é¡Œã€‚"}]

if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

# è™•ç†æª”æ¡ˆä¸Šå‚³
uploaded_file = st.file_uploader("ä¸Šå‚³ PDF æ–‡ä»¶", type=["pdf"])

def process_pdf(uploaded_file, api_key):
    if not api_key:
        st.error("è«‹å…ˆè¼¸å…¥ API Key")
        return None
    
    with st.spinner("æ­£åœ¨åˆ†ææ–‡ä»¶..."):
        # æš«å­˜æª”æ¡ˆ
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name

        # è®€å– PDF
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()
        
        # åˆ‡å‰²æ–‡æœ¬
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
        )
        texts = text_splitter.split_documents(documents)
        
        # å»ºç«‹å‘é‡åº«
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
        vector_store = FAISS.from_documents(texts, embeddings)
        
        os.remove(tmp_path) # åˆªé™¤æš«å­˜
        return vector_store

if uploaded_file and st.session_state.vector_store is None:
    if google_api_key:
        st.session_state.vector_store = process_pdf(uploaded_file, google_api_key)
        st.success("æ–‡ä»¶åˆ†æå®Œæˆï¼è«‹é–‹å§‹æå•ã€‚")
    else:
        st.warning("è«‹åœ¨å·¦å´è¼¸å…¥ Google API Key ä»¥é–‹å§‹åˆ†æã€‚")

# é¡¯ç¤ºå°è©±æ­·å²
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# è™•ç†ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input():
    if not google_api_key:
        st.info("è«‹å…ˆè¼¸å…¥ API Key")
        st.stop()
        
    if st.session_state.vector_store is None:
        st.info("è«‹å…ˆä¸Šå‚³ PDF æ–‡ä»¶")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # RAG éˆ
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=google_api_key, temperature=0.3)
    
    # å»ºç«‹ Chain
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=st.session_state.vector_store.as_retriever(search_kwargs={"k": 3}),
        memory=memory
    )
    
    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            # é€™è£¡ç‚ºäº†ç°¡å–®æ¼”ç¤ºï¼Œä¸å®Œå…¨ä½¿ç”¨ memory chain çš„æ­·å²åŠŸèƒ½ä¾†é¿å…è¤‡é›œçš„ token å•é¡Œ
            # ç›´æ¥ä½¿ç”¨ retriever æ‰¾ç­”æ¡ˆ
            docs = st.session_state.vector_store.similarity_search(prompt, k=3)
            context = "\n".join([doc.page_content for doc in docs])
            
            # çµ„è£ Prompt
            system_prompt = f"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å­¸è¡“åŠ©æ•™ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ä¸Šä¸‹æ–‡å…§å®¹å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹èª å¯¦èªªä¸çŸ¥é“ã€‚\n\nä¸Šä¸‹æ–‡ï¼š{context}\n\nå•é¡Œï¼š{prompt}"
            response = llm.invoke(system_prompt)
            
            st.write(response.content)
            st.session_state.messages.append({"role": "assistant", "content": response.content})
