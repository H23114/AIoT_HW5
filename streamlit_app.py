import streamlit as st
import os
import tempfile
import langchain

# å¼•å…¥ Google Gemini ç›¸é—œå¥—ä»¶
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

# æ ¹æ“š LangChain ç‰ˆæœ¬è‡ªå‹•åˆ¤æ–·å¼•å…¥æ–¹å¼ (ç›¸å®¹æ€§ä¿®æ­£)
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# è¨­å®šé é¢è³‡è¨Š
st.set_page_config(page_title="RAG èª²ç¨‹åŠ©ç† (Geminiç‰ˆ)", page_icon="ğŸ“š")
st.title("ğŸ“š RAG å­¸è¡“èª²ç¨‹åŠ©ç†")
st.caption("åŸºæ–¼ Google Gemini 1.5 Flash èˆ‡ LangChain çš„æª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±")

# Sidebar: API Key è¨­å®š
with st.sidebar:
    st.header("è¨­å®š")
    google_api_key = st.text_input("è¼¸å…¥ Google Gemini API Key", type="password")
    st.markdown("[å–å¾— Google API Key](https://aistudio.google.com/app/apikey)")
    st.markdown("---")
    st.write("æœ¬ç³»çµ±ç”±ç”Ÿæˆå¼ AI èª²ç¨‹å°ˆé¡Œå¯¦ä½œå»¶ä¼¸ã€‚")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼è«‹ä¸Šå‚³ä¸€ä»½ PDF è¬›ç¾©æˆ–è«–æ–‡ï¼Œæˆ‘å¯ä»¥å›ç­”ç›¸é—œå•é¡Œã€‚"}]

if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

# è™•ç†æª”æ¡ˆä¸Šå‚³
uploaded_file = st.file_uploader("ä¸Šå‚³ PDF æ–‡ä»¶", type=["pdf"])

def process_pdf(uploaded_file, api_key):
    if not api_key:
        st.error("è«‹å…ˆè¼¸å…¥ API Key")
        return None
    
    with st.spinner("æ­£åœ¨åˆ†ææ–‡ä»¶ (ä½¿ç”¨ text-embedding-004)..."):
        try:
            # æš«å­˜æª”æ¡ˆ
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_path = tmp_file.name

            # è®€å– PDF
            loader = PyPDFLoader(tmp_path)
            documents = loader.load()
            
            # åˆ‡å‰²æ–‡æœ¬
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )
            texts = text_splitter.split_documents(documents)
            
            # å»ºç«‹å‘é‡åº« (ä¿®æ­£é»ï¼šä½¿ç”¨æœ€æ–°çš„ text-embedding-004)
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004", 
                google_api_key=api_key
            )
            vector_store = FAISS.from_documents(texts, embeddings)
            
            os.remove(tmp_path) # åˆªé™¤æš«å­˜
            return vector_store
            
        except Exception as e:
            st.error(f"åˆ†ææ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None

# è§¸ç™¼æª”æ¡ˆè™•ç†
if uploaded_file and st.session_state.vector_store is None:
    if google_api_key:
        result_store = process_pdf(uploaded_file, google_api_key)
        if result_store:
            st.session_state.vector_store = result_store
            st.success("æ–‡ä»¶åˆ†æå®Œæˆï¼è«‹é–‹å§‹æå•ã€‚")
    else:
        st.warning("è«‹åœ¨å·¦å´è¼¸å…¥ Google API Key ä»¥é–‹å§‹åˆ†æã€‚")

# é¡¯ç¤ºå°è©±æ­·å²
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# è™•ç†ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input():
    if not google_api_key:
        st.info("è«‹å…ˆè¼¸å…¥ API Key")
        st.stop()
        
    if st.session_state.vector_store is None:
        st.info("è«‹å…ˆä¸Šå‚³ PDF æ–‡ä»¶")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # RAG éˆ (ä¿®æ­£é»ï¼šä½¿ç”¨ gemini-1.5-flash)
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash", 
        google_api_key=google_api_key, 
        temperature=0.3
    )
    
    # å»ºç«‹ Chain
    # ä½¿ç”¨ ConversationBufferMemory ä¾†è¨˜æ†¶å°è©±
    memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True,
        output_key="answer" # ç¢ºä¿èˆ‡ Chain çš„è¼¸å‡º key å°æ‡‰
    )
    
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=st.session_state.vector_store.as_retriever(search_kwargs={"k": 3}),
        memory=memory,
        return_source_documents=True
    )
    
    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            # é€™è£¡ç‚ºäº†é¿å… Memory èˆ‡ Streamlit é‡æ•´çš„è¡çªï¼Œæˆ‘å€‘å…ˆç°¡å–®è™•ç†
            # å¯¦éš›å°ˆæ¡ˆä¸­é€šå¸¸æœƒå°‡ memory æ”¾å…¥ session_stateï¼Œé€™è£¡ç°¡åŒ–æ¼”ç¤º
            
            response = chain.invoke({"question": prompt, "chat_history": []})
            answer = response["answer"]
            
            st.write(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})